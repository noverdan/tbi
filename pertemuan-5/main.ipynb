{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector # Import mysql.connector\n",
    "from mysql.connector import Error # Import Error from mysql.connector\n",
    "import re # Regular Expression\n",
    "import numpy as np # Numerical Python (Numpy)\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # Sastrawi Stemmer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # Sastrawi Stopword Remover\n",
    "from nltk.tokenize import word_tokenize # Natural Language Toolkit (NLTK)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "from sklearn.metrics.pairwise import cosine_similarity # Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Koneksi Ke Database dan mengambil data dari database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Connected to the database\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_config = {\n",
    "        \"host\": \"mysql-students-my-student-projects.h.aivencloud.com\",\n",
    "        \"port\": \"13993\",\n",
    "        \"user\": \"galangarsandy\",\n",
    "        \"password\": \"AVNS_1coj2um74BOgK_AGXaI\",\n",
    "        \"database\": \"quran\"\n",
    "    }\n",
    "class DatabaseConnection:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.connection = mysql.connector.connect(**db_config)\n",
    "            self.cursor = self.connection.cursor()\n",
    "            print(\"Connected to the database\\n\")\n",
    "        except Error as err:\n",
    "            print(f\"The error '{err}' occurred\")\n",
    "            \n",
    "    def Select(self, sql):\n",
    "        try:\n",
    "            self.cursor.execute(sql)\n",
    "            return self.cursor.fetchall()\n",
    "        except Error as err:\n",
    "            print(f\"Select data ERROR: \", err)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.connection.close()  \n",
    "        \n",
    "db = DatabaseConnection() # Membuat objek database connection\n",
    "data_quran = db.Select(\"SELECT * FROM id_indonesian limit 50\") # Mengambil data dari database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buat Objek stemmer dan stopwords, dan inisiasi list data corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = StemmerFactory().create_stemmer() # Membuat objek stemmer\n",
    "stopwords = StopWordRemoverFactory().create_stop_word_remover() # Membuat objek stopwords\n",
    "\n",
    "corpus = [] # List untuk menampung data quran yang sudah di pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi untuk preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    pre_process = re.sub('[^a-zA-Z]', ' ', text.lower()) # Menghilaangkan karakter selain huruf a-z dan A-Z dan diubah menjadi spasi. Lalu diubah menjadi huruf kecil\n",
    "    pre_process = re.sub(' +', ' ', pre_process) # Menghilangkan spasi yang berlebihan\n",
    "    pre_process = stopwords.remove(pre_process) # Menghilangkan stopwords atau kata-kata yang tidak penting\n",
    "    pre_process = stemmer.stem(pre_process) # Melakukan stemming atau mengubah kata-kata menjadi kata dasar\n",
    "    return pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data_quran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index, surat, ayat, text) in data_quran:\n",
    "    pre_process_data = pre_processing(text)\n",
    "    corpus.append((index, surat, ayat, pre_process_data)) # Menambahkan data ke dalam list corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vocabulary = set()\n",
    "tokenized_documents = []\n",
    "\n",
    "for (index, surat, ayat, text) in corpus:\n",
    "    tokens = word_tokenize(text) # Tokenisasi kata atau memecah kalimat menjadi per kata\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()] # Menghilangkan kata yang bukan alphabet, Misal tokens = [\"Hello\", \"world!\", \"123\", \"Python3\"] menjadi [\"hello\", \"world\", \"python\"]\n",
    "    tokenized_documents.append(\" \".join(tokens)) # Menambahkan kata-kata yang sudah di tokenisasi ke dalam list tokenized_documents\n",
    "    doc_vocabulary.update(tokens) # Menambahkan kata-kata yang unik ke dalam set vocabulary, jadi tidak ada kata yang duplikat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembobotan Kata dengan TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=doc_vocabulary) # Membuat objek vectorizer dengan parameter vocabulary yang berisi kata-kata unik\n",
    "tfidf_matrix = vectorizer.fit_transform(tokenized_documents) # Mengubah kata-kata menjadi vektor dengan metode TF-IDF, atau menghitung bobot dari masing masing kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coba untuk melakukan pencarian, dengan menggunakan cosine_similarity untuk menghitung kemiripan antara query dengan dokumen yang ada di database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Dokumen paling mirip dengan queri:\n",
       "(10, 2, 3, '(yaitu) mereka yang beriman kepada yang ghaib, yang mendirikan shalat, dan menafkahkan sebahagian rezeki yang Kami anugerahkan kepada mereka.')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = input(\"Masukkan query: \")\n",
    "query_tokens = word_tokenize(query) # Tokenisasi kata atau memecah kalimat menjadi per kata\n",
    "query_tokens = [word.lower() for word in query_tokens if word.isalpha()]\n",
    "query_vector = vectorizer.transform([\" \".join(query_tokens)]) # Mengubah kata-kata menjadi vektor\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, query_vector) # Menghitung cosine similarity antara tfidf_matrix dan query_vector\n",
    "most_similar_document_index = np.argmax(cosine_similarities) # Mengambil index yang memiliki nilai cosine similarity paling besar\n",
    "most_similar_document = data_quran[most_similar_document_index] # Mengambil data quran yang memiliki nilai cosine similarity paling besar\n",
    "\n",
    "print(\"\\nDokumen paling mirip dengan queri:\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jawaban Soal Praktikum No 1\n",
    "\n",
    "1. tokenized_documents:\n",
    "Ini adalah daftar yang berisi dokumen-dokumen yang telah diproses dan di-tokenisasi. Setiap dokumen diwakili sebagai string yang berisi kata-kata yang dipisahkan oleh spasi.\n",
    "2. doc_vocabulary:\n",
    "Ini adalah himpunan (set) yang berisi semua kata unik yang ditemukan dalam korpus dokumen setelah pemrosesan.\n",
    "3. cosine_similarities: Ini adalah array yang berisi nilai kesamaan kosinus antara query yang diberikan dan setiap dokumen dalam korpus. Nilai ini menunjukkan seberapa mirip query dengan setiap dokumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jawaban Soal Praktikum No 2\n",
    "\n",
    "Tinggal gunakan fungsi pre-processing data yang sudah dibuat\n",
    "query = pre_processing(query)\n",
    "```\n",
    "query = pre_processing(query)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Dokumen yang mirip dengan query 'Allah, maha pengasih & penyayang':\n",
       "(3, 1, 3, 'Maha Pemurah lagi Maha Penyayang.')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_query = input(\"Masukkan query: \")\n",
    "query = pre_processing(input_query) # Gunakan Pre-processing text untuk input_query\n",
    "query_tokens = word_tokenize(query) \n",
    "query_tokens = [word.lower() for word in query_tokens if word.isalpha()]\n",
    "query_vector = vectorizer.transform([\" \".join(query_tokens)])\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, query_vector) \n",
    "most_similar_document_index = np.argmax(cosine_similarities) \n",
    "most_similar_document = data_quran[most_similar_document_index]\n",
    "\n",
    "print(f\"\\nDokumen yang mirip dengan query '{input_query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jawaban Soal Praktikum No 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Dokumen-dokumen yang mirip dengan query 'maha sayang':\n",
       "(3, 1, 3, 'Maha Pemurah lagi Maha Penyayang.')\n",
       "(1, 1, 1, 'Dengan menyebut nama Allah Yang Maha Pemurah lagi Maha Penyayang.')\n",
       "(44, 2, 37, 'Kemudian Adam menerima beberapa kalimat dari Tuhannya, maka Allah menerima taubatnya. Sesungguhnya Allah Maha Penerima taubat lagi Maha Penyayang.')\n",
       "(39, 2, 32, 'Mereka menjawab: \"Maha Suci Engkau, tidak ada yang kami ketahui selain dari apa yang telah Engkau ajarkan kepada kami; sesungguhnya Engkaulah Yang Maha Mengetahui lagi Maha Bijaksana\".')\n",
       "(36, 2, 29, 'Dialah Allah, yang menjadikan segala yang ada di bumi untuk kamu dan Dia berkehendak (menciptakan) langit, lalu dijadikan-Nya tujuh langit. Dan Dia Maha Mengetahui segala sesuatu.')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_query = input(\"Masukkan query: \")\n",
    "query = pre_processing(input_query)\n",
    "query_tokens = word_tokenize(query) \n",
    "query_tokens = [word.lower() for word in query_tokens if word.isalpha()]\n",
    "query_vector = vectorizer.transform([\" \".join(query_tokens)])\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, query_vector) \n",
    "\n",
    "# Jawaban No 3\n",
    "# cosine_similarities di flatten agar menjadi 1 dimensional array\n",
    "cosine_results = cosine_similarities.flatten()\n",
    "# Mengambil index yang memiliki nilai cosine similarity lebih dari 0\n",
    "index_sorted = np.where(cosine_results > 0)[0]\n",
    "# Mengurutkan index berdasarkan nilai cosine similarity dari yang terbesar\n",
    "index_sorted = index_sorted[np.argsort(cosine_results[index_sorted])[::-1]]\n",
    "\n",
    "print(f\"\\nDokumen-dokumen yang mirip dengan query '{input_query}':\")\n",
    "for index in index_sorted:\n",
    "    print(data_quran[index])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
